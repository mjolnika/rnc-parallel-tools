{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все теги, которые могут встретиться, возможно найдется еще несколько, они неунифицированы во всем словаре."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "tags = '†|‡|nst|plupf|dem|prp|typ|neg|Phys|phys|Mus|m|emph|orn|impfct|a/adv|prtcl|inter|pn/prtcl|ab|ling|3p|dj|fin|naut|vi2/3|v_inv2|v_stat2|v_stat3|vi2|vt3|v_inv2|usu|vi1|vt1/2/3|vt1|Rus|pn/n|np|elec|nst|zoo|Saba|arc|pag/d|bio|med|vu|dim|cul|min|theat|Sov|mil|mus|ab|ac|Aj|Al|bk|ch|cj|clt|dg|el|Er|eu|f|fam|geo|geol|Gm|Gu|IG|Ig|ij|Im|imp|imper|Impf|iness|invar|io|I/P|ir|Isl|Ja|Ju|Ka|Ki|Kk|Kv|Le|lg|LI|lst|m/f|Me|Mg|Ml|nst|obl|on|padg|pad|instr|pag|pai|pa|pa|pf|pn|pr|prd|prg|Ps|psy|pt|px|Ra|sf|sj|sp|sst|st|tec|tex|Ti|Tu|UI|st|‡|Ka|Mti|†|©|Gd|hon|ecc|bot|Ps|num|mat|ma|chem|pp|ppa|onom|ab|v_inv|vs|Pol|np|leg|a|n|vn|ppp|ij|N|fp|I/P|vt|Im|Gu|dg|vi|io|px|sx|Ka|Kk|Ps|Kv|IG|vi2|prg|pag|Ju|sst|nst|pa|pn|cj|Ti|vo|lst|sst|nst|Le|plu_O|plu|v_stat|v_stat2|v_sup|v_imp|imp|Le|Mo|Ki|Mt|Aj|adv'.split('|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aberaCia aberration\n",
      "absolutizmi absolutism\n",
      "albomin album\n",
      "alebastri alabaster\n",
      "alegoria allegory\n",
      "alegoriuli allegorical\n",
      "alKimikosin alchemist\n",
      "arKidiakoni archdeacon\n",
      "arKiepiskoposi archbishop\n",
      "aGxurviloba equipment\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Запускать вот это\"\"\"\n",
    "deleteex = []\n",
    "with open('newbeautiful14.txt', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Этот кусок кода вычищает примеры употребления (: ~) и ссылки на другие лексемы (see)\n",
    "# Вычищается не все-все, а только последовательность до следующей цифры, т.к. в большинстве случаев\n",
    "# с цифры начинается новое значение, которое нужно оставить\n",
    "for t in text.splitlines():\n",
    "    t1 = re.sub('(:.+? )(\\d\\d?)', r' \\2', t)\n",
    "    t2 = re.sub('([sS]ee .+?)(\\d\\d?)', r'\\2', t1)\n",
    "    t21 = re.sub(' [23456789;†]$', '', t2)\n",
    "    t3 = re.sub(';([^;\\d]+)?~([^;$\\d]+)?', ' ', t21)\n",
    "    t4 = re.sub('\\d [^\\d]+~[^\\d]+', '', t3)\n",
    "    t5 = re.sub(':.+$', '', t4)\n",
    "    t51 = re.sub(' [23456789;†]$', '', t5)\n",
    "    deleteex.append(re.sub('[sS]ee .+?$', '', t51))\n",
    "\n",
    "# Перезаписываю файл\n",
    "with open('deleted2.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(deleteex))\n",
    "    \n",
    "with open('deleted2.txt', encoding='utf-8') as f:\n",
    "    text = f.read().splitlines()\n",
    "\n",
    "# вычищаю дублирующиеся скобки, которые помешают разбору\n",
    "# также убираю падежи\n",
    "newlines = []\n",
    "for line in text:  \n",
    "    line = re.sub('([A-z<])\\(([A-z]+)\\)([A-z-])', r'\\1\\2\\3', line)\n",
    "    line = re.sub('\\(<[^\\(\\)]+\\)', '', line) #(erg ramaC, dat rasaC, gen risaC, inst riTaC, adv radaC)\n",
    "    line = re.sub('\\((erg|dat|gen|inst|adv)[^\\(\\)]+\\)', '', line)\n",
    "    newlines.append(line)\n",
    "\n",
    "# Перезаписываю вычищенный файл\n",
    "with open('deleted3.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write('\\n'.join(newlines))\n",
    "    \n",
    "# Работаю с вычищенным   \n",
    "with open('deleted3.txt', encoding='utf-8') as f:\n",
    "    text = f.read().splitlines()    \n",
    "   \n",
    "another_turn = []\n",
    "first_dict = {}\n",
    "for line in text:\n",
    "    pr = 0\n",
    "    test = re.search('^([A-z]+) ([A-z]+)$', line)\n",
    "    if test:\n",
    "        if test.group(2) not in tags:\n",
    "            first_dict[test.group(1)] = {'transl':test.group(2), 'lex_feat':None}\n",
    "            print(line)\n",
    "            pr = 1\n",
    "    if pr == 0:\n",
    "        another_turn.append(line)\n",
    "        \n",
    "another_turn2 = []\n",
    "for line in another_turn:\n",
    "    pr = 0\n",
    "    test = re.search('^([A-z]+) ([A-z]+) ([A-z]+)$', line)\n",
    "    if test:\n",
    "        if test.group(2) in tags:\n",
    "            if test.group(3) not in tags:\n",
    "                #print(line)\n",
    "                first_dict[test.group(1)] = {'transl':test.group(3), 'lex_feat':test.group(2)}\n",
    "                #first_dict[test.group(1)] = test.group(2)\n",
    "                pr = 1\n",
    "            else:\n",
    "                pass\n",
    "    if pr == 0:\n",
    "        another_turn2.append(line)\n",
    "        \n",
    "another_turn3 = []\n",
    "for line in another_turn2:\n",
    "    items = line.split(' ')\n",
    "    test = all(m in tags for m in items[1:])\n",
    "    if test:\n",
    "        pass\n",
    "    else:\n",
    "        another_turn3.append(line)\n",
    "\n",
    "verbsadjothers = []\n",
    "turn4 = []\n",
    "vns = []  \n",
    "flexions = [] #аффиксы, которые пока отбрасываются из разбора\n",
    "\n",
    "for line in another_turn3:\n",
    "    first_tok = line.split(' ')[0]\n",
    "    if re.search('[-~],?$', first_tok) or re.search('^[-~]', first_tok) or re.search('[A-z]~~[A-z]', first_tok):\n",
    "        flexions.append(line)\n",
    "    elif re.search(' (n|N|adv|ppp|pp|pa|ij|cj|pn|np|fp|imp)[ /]', line):\n",
    "        turn4.append(line)\n",
    "    elif re.search(' vn ', line):\n",
    "        vns.append(line)\n",
    "    else:\n",
    "        verbsadjothers.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_alts(string):\n",
    "    \"\"\"Ищет ссылки на лексемы типа (<amo(s)jvreba) \"\"\"\n",
    "    alts = re.search('\\(<([A-z ,-]+?)\\)', string)\n",
    "    if alts:\n",
    "        alts = alts.group(1).split(',')\n",
    "        alts = [alt.replace(' ', '') for alt in alts]\n",
    "        alts = [re.sub(',.+', '', alt) for alt in alts]\n",
    "    return alts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vnsdict = {}\n",
    "difficult = []\n",
    "for vn in vns:\n",
    "    try:\n",
    "       \n",
    "        lexs, defs = vn.split(' vn ') \n",
    "        check = re.search('\\(<([A-z ,\\(-]+?)\\)', defs)\n",
    "        all_lexs = []\n",
    "        \n",
    "        \n",
    "     # чищу ссылки на лексемы типа (<amo(s)jvreba) от скобок\n",
    "       \n",
    "        if check: # if there are differenet forms in definitions\n",
    "            \n",
    "            change = check.group(1).replace('(', '').replace(')', '')  #fix () inside definitions\n",
    "            if change!= check.group(1):\n",
    "                defs = defs.replace(check.group(1), change)\n",
    "                defs = re.sub('([A-z])\\)([A-z])', r'\\1\\2', defs)\n",
    "        \n",
    "        # если несколько значений, буду разбирать отдельно\n",
    "        if re.search(' \\d ?', lexs):\n",
    "            difficult.append(vn)  \n",
    "        else:\n",
    "            \n",
    "            changed_lexs = re.sub(' ?\\(<([A-z][A-z][A-z][A-z].+)?\\)', r', \\1', lexs) # удаляю ссылки на лексемы типа (<amo(s)jvreba)\n",
    "            dif = changed_lexs.split(',')\n",
    "            dif = [d.replace('(', '').replace(')', '') for d in dif]\n",
    "            new_dif = []\n",
    "            # ищу теги в последовательностях\n",
    "            for d in dif:\n",
    "                dif_tags = []\n",
    "                word_parts = []\n",
    "                parts = d.strip(' ').split(' ')\n",
    "                stop = None\n",
    "                for i in range(len(parts)):\n",
    "                    if parts[-i-1] in tags and stop == None:\n",
    "                        dif_tags.append(parts[-i-1])\n",
    "                    elif stop == None:\n",
    "                        stop = -i\n",
    "                if stop == 0:\n",
    "                    word = d.strip(' ').replace(' ', '')\n",
    "                else:\n",
    "                    word = ''.join(parts[:stop])\n",
    "                new_dif.append([word, dif_tags])\n",
    "           \n",
    "              \n",
    "            alts = find_alts(defs)\n",
    "            definitions = re.split('\\d\\d?', defs)\n",
    "            # вычищаю определения от ссылок на другие лексемы\n",
    "            definitions = [re.sub('\\(<([A-z ,-]+?)\\)', '', defi) for defi  in definitions if len(re.sub('\\(<([A-z ,-]+?)\\)', '', defi)) > 3]\n",
    "           \n",
    "                \n",
    "            for d in new_dif:\n",
    "                translation = ' #def '.join(definitions).strip(' ')\n",
    "                \n",
    "                dif_tags = []\n",
    "                def_parts = []\n",
    "                parts = translation.split(' ')\n",
    "                stop = None\n",
    "                for i in range(len(parts)):\n",
    "                    if parts[i] in tags and stop == None:\n",
    "                        dif_tags.append(parts[i])\n",
    "                    elif stop == None:\n",
    "                        stop = i\n",
    "                tag_string = ' '.join(dif_tags)\n",
    "                translation = re.sub(f'^{tag_string}', '', translation)\n",
    "                \n",
    "                if len(d[1]) < 1:\n",
    "                    feats = 'vn' + ' ' + tag_string\n",
    "                else:\n",
    "                    feats = 'vn' + ' ' + ' '.join(d[1])\n",
    "                vnsdict[d[0]] = {'transl':translation.strip(' '), 'lex_feat':feats}\n",
    "            \n",
    "        \n",
    "    except ValueError:\n",
    "        difficult.append(vn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_alts2(string, position):\n",
    "    \"\"\"получение альтернативных форм глагола из скобок\"\"\"\n",
    "    if position == 0:\n",
    "        check = re.search('\\(([^\\(\\)]+), ([^\\(\\)]+)\\)', string)\n",
    "    else:\n",
    "        check = re.search('^\\(([^\\(\\)]+), ([^\\(\\)]+)\\)', string)\n",
    "    alts = []\n",
    "    if check:\n",
    "        first = check.group(1).strip(' ')\n",
    "        second = check.group(2).strip(' ')\n",
    "        if re.search(' ?no ', first):\n",
    "            first = None\n",
    "        elif re.search(' ?no ', second):\n",
    "            second = None\n",
    "        if first:\n",
    "            alts.append(first)\n",
    "        if second:\n",
    "            alts.append(second)\n",
    "    if alts:\n",
    "        return alts\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбираю сложные случаи с vn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\mjo\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\ipykernel_launcher.py:5: FutureWarning: Possible set difference at position 8\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "for line in difficult:\n",
    "    \n",
    "    # чищу от лишнего (упоминания какого лица глагол ит.д)\n",
    "    line = re.sub('([A-z])\\(([A-z]+)\\)([A-z])', r'\\1\\2\\3', line)\n",
    "    line = re.sub('\\(<[A-z --,\\.]+\\) ', ' ', line)\n",
    "    line = re.sub(' \\([1|2]p .+?\\)(,)', r'\\1', line)\n",
    "\n",
    "    parts = re.split(' \\d\\d? ', line)\n",
    "  #  print(parts)\n",
    "    alternatives = []\n",
    "    new_parts = []\n",
    "    for n, part in enumerate(parts):\n",
    "        new_part = part.strip(' ')\n",
    "        toks = part.split(' ')\n",
    "        stop = None\n",
    "        found_tags = []\n",
    "        for tok in toks:\n",
    "            if tok in tags and stop == None:\n",
    "                new_part = re.sub(f'^{tok} ?', '', new_part)\n",
    "                found_tags.append(tok)\n",
    "            elif stop == None:\n",
    "                stop = 1\n",
    "        #alts = find_alts(new_part)\n",
    "        #new_part = re.sub(' ?\\(<([A-z][A-z][A-z][A-z].+)?\\)', r', \\1', new_part)\n",
    "        \n",
    "        alts2 = find_alts2(new_part, n)\n",
    "        if alts2:\n",
    "           # print(alts2)\n",
    "            for alt in alts2:\n",
    "                alternatives.append(alt)\n",
    "                if n == 0:\n",
    "                    new_part = re.sub(' ?\\(([^\\(\\)]+),([^\\(\\)]+)\\)', '', new_part)\n",
    "                else:\n",
    "                    new_part = re.sub('^ ?\\(([^\\(\\)]+),([^\\(\\)]+)\\)', '', new_part)\n",
    "        part = ' '.join(found_tags) + ' ' + new_part\n",
    "        new_parts.append(part.strip(' '))\n",
    " #   print('*'*10,new_parts, alternatives)\n",
    "    lex_parts = new_parts[0].split(' ')\n",
    "    \n",
    "    dif_tags = []\n",
    "\n",
    "    stop = None\n",
    "    for i in range(len(lex_parts)):\n",
    "        if lex_parts[-i-1] in tags and stop == None:\n",
    "            dif_tags.append(lex_parts[-i-1])\n",
    "        elif stop == None:\n",
    "            stop = -i\n",
    "    tag_string = ' '.join(dif_tags)\n",
    "    \n",
    "    for tag in dif_tags:\n",
    "        new_parts[0] = re.sub(f' ?{tag} ?', '', new_parts[0])\n",
    "    \n",
    "    definitions = [re.sub('^ ?vn ?', '', p) for p in new_parts[1:]]\n",
    "    translation = ' #def '.join(definitions).strip(' ')\n",
    "                \n",
    "    \n",
    "\n",
    "    feats = 'vn' + ' ' + tag_string\n",
    "   \n",
    "    vnsdict[new_parts[0]] = {'transl':translation.strip(' '), 'lex_feat':feats}\n",
    "    if alts2:        \n",
    "        for alt in alts2:\n",
    "                vnsdict[alt] = {'transl':translation.strip(' '), 'lex_feat':feats}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15657"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vnsdict) #15657"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import json\n",
    "#fname = 'parsedvns.json'\n",
    "#with open(fname, 'w', encoding='utf-8') as f:\n",
    "#    json.dump(vnsdict, f, indent = 6, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### turn4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "! Возможно, что для скобки в строке нужно заменять не так string = string.strip(' ').replace('(', '').replace(')', '')\n",
    "\n",
    "А так:\n",
    "```\n",
    "string = string.strip(' ')\n",
    "string = re.sub('([A-z])\\) ', r'\\1 ', string)\n",
    "string = re.sub(' \\(([A-z])', r' \\1', string)\n",
    "```\n",
    "\n",
    "Но я пока не решила, т.к. есть слова типа ar(a), и там скобку нужно оставить, в отличие от, например, (sst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_tags(string):\n",
    "    \"\"\"Выделение из подстроки, определенной как лексема, тегов \"\"\"\n",
    "    string = string.strip(' ').replace('(', '').replace(')', '')\n",
    "    toks = string.split(' ')\n",
    "    stop = None\n",
    "    found_tags = []\n",
    "    for i in range(len(toks)-1):\n",
    "        if toks[-i-1] in tags and stop == None:\n",
    "            found_tags.append(toks[-i-1])\n",
    "        elif stop == None:\n",
    "            stop = -i\n",
    "    if stop == 0:\n",
    "        lex = ''.join(toks)\n",
    "    elif stop == None:\n",
    "        lex = toks[0]\n",
    "    else:\n",
    "        lex = ''.join(toks[:stop])\n",
    "    \n",
    "    # чищу лексему в словаре от лишних символов (все, кроме букв, *, [], () удаляется)\n",
    "    lex = re.sub('[^A-z-*\\[\\]\\(\\)]', '', lex)\n",
    "    return lex, found_tags[::-1]       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_other_definitions(lex, other_defs):\n",
    "    variants = lex.split(',')\n",
    "    found_vars_tags = []\n",
    "    for var in variants:\n",
    "        var = var.replace('!', ',')\n",
    "        \n",
    "        v, t = find_tags(var)\n",
    "        found_vars_tags.append([v, t])\n",
    "        \n",
    "    for v, t in found_vars_tags:\n",
    "        if t:\n",
    "            lex_f = ' '.join(t)\n",
    "        else:\n",
    "            lex_f = None\n",
    "        translation = ' #def '.join(other_defs).strip(' ')\n",
    "        \n",
    "        first_dict[v] = {'transl':translation, 'lex_feat':lex_f}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_line(string):\n",
    "    # если попался аффикс, напечатаем его. Большинство мы отсеяли в переменную flexions\n",
    "    if re.search('[^A-z][ps]x[^A-z]', string):\n",
    "        print('AFFIXSTRING:', string)\n",
    "    toks = string.split(' ')\n",
    "    start = None\n",
    "    stop = None\n",
    "    found_tags = []\n",
    "    lexemes = []\n",
    "    for i in range(len(toks)):\n",
    "        if toks[i] not in tags and stop == None and start == None:\n",
    "            lexemes.append(toks[i])\n",
    "        elif toks[i] in tags and stop == None and start == None:\n",
    "            start = i\n",
    "            found_tags.append(toks[i])\n",
    "        elif toks[i] in tags and stop == None and start != None:\n",
    "            found_tags.append(toks[i])\n",
    "        elif toks[i] not in tags and stop == None and start != None:\n",
    "            stop = i\n",
    "    lexs = ''.join(lexemes)\n",
    "    definitions = ' '.join(toks[stop:])\n",
    "    tag_string = ' '.join(found_tags)\n",
    "\n",
    "    bracketes = re.findall('\\(([^\\(\\)]+)\\)', lexs)\n",
    "    for b in bracketes:\n",
    "        if b not in tags:\n",
    "            lexs = re.sub(f'\\(({b})\\)', r'\\1', lexs)\n",
    "    \n",
    "    lexs = lexs.replace('(', ' (').replace('!', ',')\n",
    " \n",
    "    variants = lexs.split(',')\n",
    "    forreturn = ''\n",
    "    found_vars_tags = []\n",
    "    for var in variants:\n",
    "        v, t = find_tags(var)\n",
    "        found_vars_tags.append([v, t])\n",
    "        for v, t in found_vars_tags:\n",
    "\n",
    "            if t:\n",
    "                lex_f = tag_string + ' ' + ' '.join(t)\n",
    "            else:\n",
    "                lex_f = tag_string\n",
    "            translation = definitions\n",
    "            \n",
    "            # чистка от вхождения типа 'lexeme ... 1 definition 2', т.е. когда обрывается определение\n",
    "            if not re.sub('2+$', '', translation):\n",
    "                first_dict[v] = {'transl':translation, 'lex_feat':lex_f}\n",
    "                forreturn += f'|||{v}|||{translation}|||{lex_f}\\n'\n",
    "            else:\n",
    "                first_dict[v] = {'transl':re.sub('2+$', '', translation), 'lex_feat':lex_f}\n",
    "                forreturn += f'|||{v}|||{translation}|||{lex_f}\\n'\n",
    "            \n",
    "            if re.search('[^A-z][ps]x[^A-z]', string):\n",
    "                print(v, '|transl|', translation, '|lex_f|', lex_f)\n",
    "    return forreturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "difficultdigits = [] # выбрасываю строки с ошибками в порядке значений 1 2 2 3 или строки с цифровыми значениями (88 литров)\n",
    "turn5 = []\n",
    "for turn in turn4:\n",
    "    digits = re.findall(' \\d\\d? ', turn)\n",
    "    summa = 0\n",
    "    for i in range(len(digits)):\n",
    "        summa += i+1\n",
    "    if summa != sum([int(d) for d in digits]):\n",
    "        difficultdigits.append(turn)\n",
    "    else:\n",
    "        turn5.append(turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFFIXSTRING: arKe(~) n px ‡ chief, leader, arch~\n",
      "arKe |transl| chief, leader, arch~ |lex_f| n px ‡\n",
      "AFFIXSTRING: xkva(~) n px nst wits, mind \n",
      "xkva |transl| wits, mind  |lex_f| n px nst\n",
      "AFFIXSTRING: Hanmeti n a Khanmet’i (middle stage of Old Georgian c 600 AD, when H- px marked verb’s 2p S & 3p IO) \n",
      "Hanmeti |transl| Khanmet’i (middle stage of Old Georgian c 600 AD, when H- px marked verb’s 2p S & 3p IO)  |lex_f| n a\n",
      "AFFIXSTRING: haemeti n (in) early Old Georgian (haemet’i, extra ‘h’, h- as px marking 2p S and 3p IO of all verbs) \n",
      "haemeti |transl| (in) early Old Georgian (haemet’i, extra ‘h’, h- as px marking 2p S and 3p IO of all verbs)  |lex_f| n\n"
     ]
    }
   ],
   "source": [
    "for turn in turn5:\n",
    "    parts = re.split(' \\d\\d? ', turn)\n",
    "    lex = parts[0]\n",
    "    other_defs = parts[1:]\n",
    "    \n",
    "    if other_defs:\n",
    "        with_other_definitions(lex, other_defs)\n",
    "       \n",
    "            \n",
    "    else: ## если только одно значение в строке\n",
    "        split_line(lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "other = []\n",
    "verbs = []\n",
    "iofp = []\n",
    "\n",
    "for item in verbsadjothers:\n",
    "    if re.search(' (io|fp) ', item) and not re.search(' v[istco_]', item):\n",
    "        iofp.append(item)\n",
    "    elif not re.search(' v[istco_][^A-z]', item) and not re.search(' aor ', item) and not re.search(' will ', item):\n",
    "        other.append(item)\n",
    "    else:\n",
    "        verbs.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for turn in iofp:\n",
    "    parts = re.split(' \\d\\d? ', turn)\n",
    "    lex = parts[0]\n",
    "    other_defs = parts[1:]\n",
    "    \n",
    "    if other_defs:\n",
    "        with_other_definitions(lex, other_defs)\n",
    "    else: \n",
    "        split_line(lex)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "verbforms = []\n",
    "other2 = []\n",
    "adjdifficultdigits = []\n",
    "adjlen1 = []\n",
    "notags = []  # нет тегов в строке\n",
    "gooddigits = []\n",
    "eba = []  # скорее всего строки vn, в которых по каким-то причинам отсутствует тег vn\n",
    "for o in other:\n",
    "    if re.search('^[A-z\\[\\]\\(\\)]+(s|vs|bian|eva)[ ,]', o):\n",
    "       verbforms.append(o)\n",
    "    elif re.search('^[A-z\\[\\]\\(\\)]+eba[ ,]', o):\n",
    "        if 'a' in o.split(' ')[:6]:  # если в начале встречается тег прилагательного, не буду кидать это слово к герундиям-vn\n",
    "            other2.append(o)\n",
    "        else:\n",
    "            eba.append(o)    \n",
    "    elif re.search('[\\(\\)A-z-]+ +\\([\\(\\)A-z-]+, [\\(\\)A-z-]+\\)', o.replace('1', ' ').replace('(†)', ' ')):\n",
    "        first_occ = re.search('([\\(\\)A-z-]+) +\\([\\(\\)A-z-]+, [\\(\\)A-z-]+\\)', o.replace('1', ' ').replace('(†)', ' ')).group(1)\n",
    "        if len(first_occ) < 2 and first_occ != 'a':\n",
    "            verbforms.append(o)\n",
    "        elif re.search('[A-z\\[\\]\\(\\)]+(s|ba|vs|bian)$', first_occ):\n",
    "            verbforms.append(o)\n",
    "        else:\n",
    "            other2.append(o)\n",
    "    \n",
    "    elif re.search('(fut|pres|aor|imp|2p|IO|3p|pf)[^A-z]', o) and not re.search(' [p|s]x ', o):\n",
    "        verbforms.append(o)\n",
    "    else:\n",
    "       # print(o)\n",
    "        found_tags = []\n",
    "        toks = o.split(' ')[:7]\n",
    "        for tok in toks:\n",
    "            if tok.replace('(', '').replace(')', '') in tags:\n",
    "                found_tags.append(tok)\n",
    "        digits = re.findall('\\d\\d?', o)\n",
    "        summa = 0\n",
    "        for i in range(len(digits)):\n",
    "            summa += i+1\n",
    "        if summa == sum([int(d) for d in digits]) and summa > 0:\n",
    "            gooddigits.append(o)\n",
    "        elif summa != sum([int(d) for d in digits]):\n",
    "            if len(digits)==1:\n",
    "                adjlen1.append(o)\n",
    "            else:\n",
    "                adjdifficultdigits.append(o)\n",
    "        \n",
    "        \n",
    "        elif len(found_tags) == 0:\n",
    "            notags.append(o)\n",
    "        else:\n",
    "            other2.append(o)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разберусь с presumbaly герундиями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in eba:\n",
    "    \n",
    "    if re.search('^([A-z-]+) \\(([A-z-]+), ([A-z -]+)\\)', line):\n",
    "        query = re.search('^([A-z-]+) \\(([A-z-]+), ([A-z -]+)\\)', line)\n",
    "        lexemes = [query.group(1), query.group(2)]\n",
    "        if query.group(3) != 'no pf':\n",
    "            lexemes.append(query.group(3))\n",
    "        definitions = line.replace(query.group(0), '')\n",
    "        \n",
    "        if len(re.split(' \\d\\d? ', definitions)) >1:\n",
    "            definitions = re.split(' \\d\\d? ', definitions)\n",
    "        else:\n",
    "            defs = []\n",
    "            defs.append(definitions)\n",
    "            definitions = defs\n",
    "            \n",
    "        f_tags = []\n",
    "        stop = None\n",
    "        tokens = definitions[0].strip(' ').split(' ')\n",
    "        for n, token in enumerate(tokens):\n",
    "            if token in tags and stop == None:\n",
    "                f_tags.append(token)\n",
    "            elif stop == None:\n",
    "                stop = n\n",
    "        definitions[0] = ' '.join(tokens[stop:])\n",
    "       # print(definitions)\n",
    "        translation = ' #def '.join(definitions)\n",
    "        for lexeme in lexemes:\n",
    "            if translation:\n",
    "                first_dict[lexeme] = {'transl': translation, 'lex_feat': ' '.join(tags)}\n",
    "    else:\n",
    "        \n",
    "        if len(re.split(' \\d\\d? ', line)) == 1:\n",
    "            other2.append(line)\n",
    "        else:\n",
    "            parts = re.split(' \\d\\d? ', line)\n",
    "\n",
    "            alternatives = []\n",
    "            new_parts = []\n",
    "            for n, part in enumerate(parts):\n",
    "                new_part = part.strip(' ')\n",
    "                toks = part.split(' ')\n",
    "                stop = None\n",
    "                found_tags = []\n",
    "                for tok in toks:\n",
    "                    if tok in tags and stop == None:\n",
    "                        new_part = re.sub(f'^{tok} ?', '', new_part)\n",
    "                        found_tags.append(tok)\n",
    "                    elif stop == None:\n",
    "                        stop = 1\n",
    "                \n",
    "                \n",
    "                alts2 = find_alts2(new_part, n)\n",
    "                if alts2:\n",
    "                    for alt in alts2:\n",
    "                        alternatives.append(alt)\n",
    "                        if n == 0:\n",
    "                            new_part = re.sub(' ?\\(([^\\(\\)]+),([^\\(\\)]+)\\)', '', new_part)\n",
    "                        else:\n",
    "                            new_part = re.sub('^ ?\\(([^\\(\\)]+),([^\\(\\)]+)\\)', '', new_part)\n",
    "                part = ' '.join(found_tags) + ' ' + new_part\n",
    "                new_parts.append(part.strip(' '))\n",
    "            lex_parts = new_parts[0].split(' ')\n",
    "            \n",
    "            dif_tags = []\n",
    "\n",
    "            stop = None\n",
    "            for i in range(len(lex_parts)):\n",
    "                if lex_parts[-i-1] in tags and stop == None:\n",
    "                    dif_tags.append(lex_parts[-i-1])\n",
    "                elif stop == None:\n",
    "                    stop = -i\n",
    "            tag_string = ' '.join(dif_tags)\n",
    "            \n",
    "            for tag in dif_tags:\n",
    "                new_parts[0] = re.sub(f' ?{tag} ?', '', new_parts[0])\n",
    "            \n",
    "            \n",
    "            translation = ' #def '.join([defi for defi in definitions if len(defi)>0]).strip(' ')\n",
    "                        \n",
    "            \n",
    "    \n",
    "            feats = tag_string\n",
    "            if translation:\n",
    "                first_dict[new_parts[0]] = {'transl':translation.strip(' '), 'lex_feat':feats}\n",
    "               # print(new_parts[0], translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFFIXSTRING: (-)dagvarad sx hyphenated after gen of noun similarly; accordingly\n",
      "-dagvarad |transl| hyphenated after gen of noun similarly; accordingly |lex_f| sx\n"
     ]
    }
   ],
   "source": [
    "for turn in other2:\n",
    "    parts = re.split(' \\d\\d? ', turn)\n",
    "    lex = parts[0]\n",
    "    other_defs = parts[1:]\n",
    "    \n",
    "    if other_defs:\n",
    "        with_other_definitions(lex, other_defs)\n",
    "    else:\n",
    "        split_line(lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for turn in adjlen1:\n",
    "    lex = turn\n",
    "    other_defs = None\n",
    "    if other_defs:\n",
    "        with_other_definitions(lex, other_defs)\n",
    "    else:\n",
    "        split_line(lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for turn in gooddigits:\n",
    "    parts = re.split(' \\d\\d? ', turn)\n",
    "    lex = parts[0]\n",
    "    other_defs = parts[1:]\n",
    "    \n",
    "    if other_defs:\n",
    "        with_other_definitions(lex, other_defs)\n",
    "    else:\n",
    "        split_line(lex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85276"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_dict) #83968"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3341"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verbforms) #2865"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(adjdifficultdigits) #71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "491"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(difficultdigits) #491"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for turn in difficultdigits:\n",
    "    \n",
    "    digits = re.findall(' \\d\\d? ', turn)\n",
    "    summa = 0\n",
    "    digits = [int(d) for d in digits]\n",
    "    dif = []\n",
    "    accepted = [0, -1, -2]\n",
    "    \n",
    "    if len(digits)>1:\n",
    "        \n",
    "        for i in range(len(digits)-1):\n",
    "           \n",
    "            dif.append(digits[i] - digits[i+1])\n",
    "            \n",
    "    check = all([d in accepted for d in set(dif)]) and dif\n",
    "    \n",
    "    if len(digits) == 1 and digits[0] == 2 and not re.search('kopecs', turn):\n",
    "        \n",
    "        new_t = turn.replace(' 2 ', ' #def ')\n",
    "        new_t = re.sub('#def $', '', new_t)\n",
    "        split_line(new_t)\n",
    "        \n",
    "    elif check:\n",
    "        parts = re.split(' \\d ', turn)\n",
    "        lex = parts[0].strip(' ')\n",
    "        other_defs = parts[1:]\n",
    "        \n",
    "        \n",
    "        lex_toks = lex.split(' ')\n",
    "        if len(lex_toks) >1:\n",
    "            \n",
    "            lexeme_end = 0\n",
    "            tag_start = 0\n",
    "            ended = 0\n",
    "            \n",
    "            for n, tok in enumerate(lex_toks):\n",
    "                \n",
    "\n",
    "                \n",
    "                if tok in tags and tag_start == 0:\n",
    "                    #print(tok, n)\n",
    "                    lexeme_end = n\n",
    "                    tag_start = 1\n",
    "                elif tok in tags and ended == 0:\n",
    "                    lexeme_end = n\n",
    "                elif tok not in tags and tag_start == 1 and ended == 0:\n",
    "                    lexeme_end = n\n",
    "                    ended = 1\n",
    "    \n",
    " \n",
    "            lex = ' '.join(lex_toks[:lexeme_end+1])\n",
    "        \n",
    "          \n",
    "            try:\n",
    "                parts[1] = ' '.join(lex_toks[lexeme_end+1:]) + ' ' + parts[1]\n",
    "            except IndexError:\n",
    "                parts.append(' '.join(lex_toks[lexeme_end+1:]) + ' ')\n",
    "\n",
    "        \n",
    "        \n",
    "        if other_defs:\n",
    "           # print('checked2:', turn)\n",
    "            variants = lex.split(',')\n",
    "            found_vars_tags = []\n",
    "            for var in variants:\n",
    "                var = var.replace('!', ',')\n",
    "                v, t = find_tags(var)\n",
    "                found_vars_tags.append([v, t])\n",
    "            for v, t in found_vars_tags:\n",
    "                if t:\n",
    "                    lex_f = ' '.join(t)\n",
    "                else:\n",
    "                    lex_f = None\n",
    "                translation = ' #def '.join(other_defs).strip(' ')\n",
    "                #print(turn, '|||', v, '|||', translation, '|||', lex_f)\n",
    "                first_dict[v] = {'transl':translation, 'lex_feat':lex_f}\n",
    "        \n",
    "\n",
    "    elif dif:\n",
    "        parts = re.split('[ ;][12345789] ', turn)\n",
    "        if not len(parts)>1:\n",
    "            split_line(turn)\n",
    "        else:\n",
    "            lex = parts[0]\n",
    "            other_defs = parts[1:]\n",
    "            if other_defs:\n",
    "                variants = lex.split(',')\n",
    "                found_vars_tags = []\n",
    "                for var in variants:\n",
    "                    var = var.replace('!', ',')\n",
    "                    v, t = find_tags(var)\n",
    "                    found_vars_tags.append([v, t])\n",
    "                for v, t in found_vars_tags:\n",
    "                    if t:\n",
    "                        lex_f = ' '.join(t)\n",
    "                    else:\n",
    "                        lex_f = None\n",
    "                    translation = ' #def '.join(other_defs).strip(' ')\n",
    "                    \n",
    "                    first_dict[v] = {'transl':translation, 'lex_feat':lex_f}\n",
    "    \n",
    "    #print(turn)\n",
    "    else:\n",
    "        split_line(turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101438"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_dict) + len(vnsdict) #100130"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = 0\n",
    "kv = 0\n",
    "for turn in adjdifficultdigits:\n",
    "    \n",
    "    digits = re.findall(' \\d\\d? ', turn)\n",
    "    summa = 0\n",
    "    digits = [int(d) for d in digits]\n",
    "    dif = []\n",
    "    accepted = [0, -1, -2]\n",
    "    \n",
    "    if len(digits)>1:\n",
    "        for i in range(len(digits)-1):\n",
    "            dif.append(digits[i] - digits[i+1])\n",
    "            \n",
    "    check = all([d in accepted for d in set(dif)]) and dif\n",
    "    \n",
    "    if len(digits) == 1 and digits[0] == 2 and not re.search('kopecs', turn):\n",
    "        new_t = turn.replace(' 2 ', ' #def ')\n",
    "        split_line(new_t)\n",
    "        print(split_line(new_t))\n",
    "        \n",
    "        \n",
    " ##############################################################       \n",
    "    elif check:\n",
    "        parts = re.split(' \\d ', turn)\n",
    "        lex = parts[0].strip(' ')\n",
    "     #   print(lex)\n",
    "        other_defs = parts[1:]\n",
    "        \n",
    "        \n",
    "        lex_toks = lex.split(' ')\n",
    "        if len(lex_toks) >1:\n",
    "            \n",
    "            lexeme_end = 0\n",
    "            tag_start = 0\n",
    "            ended = 0\n",
    "            for n, tok in enumerate(lex_toks):\n",
    "\n",
    "                if tok in tags and tag_start == 0:\n",
    "                    #print(tok, n)\n",
    "                    lexeme_end = n\n",
    "                    tag_start = 1\n",
    "                elif tok in tags and ended == 0:\n",
    "                    lexeme_end = n\n",
    "                elif tok not in tags and tag_start == 1 and ended == 0:\n",
    "                    lexeme_end = n\n",
    "                    ended = 1\n",
    "          #  print(lexeme_end)   \n",
    "            lex = ' '.join(lex_toks[:lexeme_end+1])\n",
    "        \n",
    "        \n",
    " ###################################################################       \n",
    "            try:\n",
    "                parts[1] = ' '.join(lex_toks[lexeme_end+1:]) + ' ' + parts[1]\n",
    "            except IndexError:\n",
    "                parts.append(' '.join(lex_toks[lexeme_end+1:]) + ' ')\n",
    "\n",
    "        if other_defs:\n",
    "            variants = lex.split(',')\n",
    "            found_vars_tags = []\n",
    "            for var in variants:\n",
    "                var = var.replace('!', ',')\n",
    "                v, t = find_tags(var)\n",
    "                found_vars_tags.append([v, t])\n",
    "            for v, t in found_vars_tags:\n",
    "                if t:\n",
    "                    lex_f = ' '.join(t)\n",
    "                else:\n",
    "                    lex_f = None\n",
    "                translation = ' #def '.join(other_defs).strip(' ')\n",
    "                #print(turn, '|||', v, '|||', translation, '|||', lex_f)\n",
    "                first_dict[v] = {'transl':translation, 'lex_feat':lex_f}\n",
    "               # print(v, first_dict[v])\n",
    "   # elif all([d for d in set(dif) in accepted]):\n",
    "   #     print(t)\n",
    "    elif dif:\n",
    "        parts = re.split('[ ;][12345789] ', turn)\n",
    "        if not len(parts)>1:\n",
    "            split_line(turn)\n",
    "            print(split_line(turn))\n",
    "        else:\n",
    "            lex = parts[0]\n",
    "            other_defs = parts[1:]\n",
    "            if other_defs:\n",
    "                variants = lex.split(',')\n",
    "                found_vars_tags = []\n",
    "                for var in variants:\n",
    "                    var = var.replace('!', ',')\n",
    "                    v, t = find_tags(var)\n",
    "                    found_vars_tags.append([v, t])\n",
    "                for v, t in found_vars_tags:\n",
    "                    if t:\n",
    "                        lex_f = ' '.join(t)\n",
    "                    else:\n",
    "                        lex_f = None\n",
    "                    translation = ' #def '.join(other_defs).strip(' ')\n",
    "                    \n",
    "                    first_dict[v] = {'transl':translation, 'lex_feat':lex_f}\n",
    "                   \n",
    "    else:\n",
    "        split_line(turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in notags:\n",
    "    tokens = line.strip(' ').split(' ')\n",
    "    lex_tokens = []\n",
    "    def_tokens = []\n",
    "    border = None\n",
    "    bordered = False\n",
    "    for n, token in enumerate(tokens):\n",
    "        if not re.search('[,!]$', token) and bordered == False:\n",
    "            border = n\n",
    "            bordered = True\n",
    "    lexemes = tokens[:border+1]\n",
    "    defs = tokens[border+1:]\n",
    "    translation = ' '.join([defi for defi in defs if len(defi)>0])\n",
    "    for lexeme in lexemes:\n",
    "        first_dict[lexeme] = {'transl':translation, 'lex_feat':\"\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не забыть добавить эту строку, не помню по какой причине, но это в словарь не попадало"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_dict['eli'] = {'transl':'n bot © azalea #def N Turkic nomadic tribe; #def n † land, country; #def † herd of cattle',\n",
    "                     'lex_feat':\"\"}\n",
    "first_dict.update(vnsdict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_filtered = {}\n",
    "\n",
    "for key in first_dict.keys():\n",
    "    if not first_dict[key]['transl'] == '':\n",
    "        if first_dict[key]['transl'] == None:\n",
    "            first_dict[key]['transl'] = \"\"\n",
    "        elif first_dict[key]['lex_feat'] == None:\n",
    "            first_dict[key]['lex_feat'] = \"\"\n",
    "        first_dict[key]['transl'] = first_dict[key]['transl'].strip(' ')\n",
    "        first_dict[key]['lex_feat'] = first_dict[key]['lex_feat'].strip(' ')\n",
    "        first_filtered[key] = first_dict[key]\n",
    "first_dict = first_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101550"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(first_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "fname = 'parsed11.json'\n",
    "with open(fname, 'w', encoding='utf-8') as f:\n",
    "    json.dump(first_dict, f, indent = 6, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFFIXSTRING: arKe(~) n px ‡ chief, leader, arch~\n",
      "arKe |transl| chief, leader, arch~ |lex_f| n px ‡\n",
      "AFFIXSTRING: xkva(~) n px nst wits, mind \n",
      "xkva |transl| wits, mind  |lex_f| n px nst\n",
      "AFFIXSTRING: Hanmeti n a Khanmet’i (middle stage of Old Georgian c 600 AD, when H- px marked verb’s 2p S & 3p IO) \n",
      "Hanmeti |transl| Khanmet’i (middle stage of Old Georgian c 600 AD, when H- px marked verb’s 2p S & 3p IO)  |lex_f| n a\n",
      "AFFIXSTRING: haemeti n (in) early Old Georgian (haemet’i, extra ‘h’, h- as px marking 2p S and 3p IO of all verbs) \n",
      "haemeti |transl| (in) early Old Georgian (haemet’i, extra ‘h’, h- as px marking 2p S and 3p IO of all verbs)  |lex_f| n\n"
     ]
    }
   ],
   "source": [
    "for turn in turn5:\n",
    "    parts = re.split(' \\d\\d? ', turn)\n",
    "    lex = parts[0]\n",
    "    other_defs = parts[1:]\n",
    "    \n",
    "    if other_defs:\n",
    "        with_other_definitions(lex, other_defs)\n",
    "       \n",
    "            \n",
    "    else: ## если только одно значение в строке\n",
    "        split_line(lex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ура, молодец, распарсила почти весь словарь! Не у дел только:\n",
    "\n",
    "verbforms\n",
    "\n",
    "verbs\n",
    "\n",
    "flexions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### convert georgian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "alp ='''aა\n",
    "bბ\n",
    "gგ\n",
    "dდ\n",
    "eე\n",
    "vვ\n",
    "zზ\n",
    "**Eჱ\n",
    "Tთ\n",
    "iი\n",
    "kკ\n",
    "lლ\n",
    "mმ\n",
    "nნ\n",
    "**yჲ\n",
    "oო\n",
    "pპ\n",
    "Zჟ\n",
    "rრ\n",
    "sს\n",
    "tტ\n",
    "uუ\n",
    "**wჳ\n",
    "Pფ\n",
    "Kქ\n",
    "Gღ\n",
    "qყ\n",
    "Sშ\n",
    "Xჩ\n",
    "Cც\n",
    "jძ\n",
    "cწ\n",
    "xჭ\n",
    "Hხ\n",
    "**Qჴ\n",
    "Jჯ\n",
    "hჰ\n",
    "**Oჵ'''\n",
    "alpdict = {}\n",
    "for string in alp.splitlines():\n",
    "    trans, georg = list(string.strip('*'))\n",
    "    alpdict[trans] = georg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Конвертирую словарь по правилам национального корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "georg_dict = {}\n",
    "for key in first_dict.keys():\n",
    "    transl = ''\n",
    "    for i in list(key):\n",
    "        try:\n",
    "            t = alpdict[i]\n",
    "        except KeyError:\n",
    "            t = i\n",
    "        transl += t\n",
    "    tran = first_dict[key]['transl']\n",
    "    tr1 = re.sub('^ ?#def ?', '', tran)\n",
    "    tr2 = re.sub(' ?#def ?$', '', tr1)\n",
    "    first_dict[key]['transl'] = tr2\n",
    "    georg_dict[transl] = first_dict[key]\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'parsed12.json'\n",
    "with open(fname, 'w', encoding='utf-8') as f:\n",
    "    json.dump(georg_dict, f, indent = 6, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отдельно словарь без пунктуации, т.к. где-то расхождения между парсером Мойера и словарем, и преугадать не всегда получится."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_punct_dicti = {}\n",
    "for key in georg_dict.keys():\n",
    "    newkey = ''\n",
    "    for l in list(key):\n",
    "        if l.isalpha():\n",
    "            newkey += l\n",
    "    no_punct_dicti[newkey] = georg_dict[key]\n",
    "with open('parsed12_nopunct.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(no_punct_dicti, f, indent = 6, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
